<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Transformer</title>
      <link href="/2021/12/01/transformer/"/>
      <url>/2021/12/01/transformer/</url>
      
        <content type="html"><![CDATA[<p>本项目使用pytorch框架实现<a href="https://arxiv.org/abs/1706.03762">transformer</a>[1]</p><h2 id="1-数据集和数据处理"><a href="#1-数据集和数据处理" class="headerlink" title="1.数据集和数据处理"></a>1.数据集和数据处理</h2><p>使用预处理好的IWSLT’14 De-En数据集,源语言德语目标语言英语,使用双字节编码分词.<br>先写一个Tokenizer类生成字典,并且有以下功能<br>1.读取bpe表生成word2idx_dict,idx2word_dict<br>2.句子编码,词表里没有的词用<unk>替换<br>3.句子解码,用上面的字典idx2word<br>4.padding,padding后输入进<br>5.depadding<br>4.debpe过程<br>然后用torch.utils实现一个自己的DataSet,DataLoader就行了</p><h2 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h2><p>根据教材<a href="https://github.com/NiuTrans/MTBook">2</a>的介绍,循环神经网络每个循环单元都有向前依赖性,也就是当前时间步的处理依赖前一时间步处理的结果。这个性质可以使序列的“历史”信息不断被传递,但是也造成模型运行效率的下降。特别是对于自然语言处理任务, 序列往往较长,无论是传统的 RNN 结构,还是更为复杂的 LSTM 结构,都需要很多次循环单元的处理才能够捕捉到单词之间的长距离依赖。由于需要多个循环单元的处理,距离较远的两个单词之间的信息传递变得很复杂.</p><p>Transformer 模型仅仅使用自注意力机制和标准的前馈神经网络,完全不依赖任何循环单元或者卷积操作。自注意力机制的优点在于可以直接对序列中任意两个单元之间的关系进行建模,这使得长距离依赖等问题可以更好地被求解。</p><p>下图展示了 Transformer 的结构。编码器由若干层组成(绿色虚线框就代表一层) 。每一层(Layer)的输入都是一个向量序列,输出是同样大小的向量序列,而Transformer 层的作用是对输入进行进一步的抽象,得到新的表示结果。不过这里的层并不是指单一的神经网络结构,它里面由若干不同的模块组成.</p><p><img src="./src/Transformer.jpeg" alt="Transformer结构图"></p><p>主要包括下列层:</p><ul><li>嵌入层和位置编码</li><li>自注意力表示层(多头注意力机制)</li><li>前馈神经网络</li><li>残差连接和层标准化</li></ul><h3 id="嵌入层和位置编码层"><a href="#嵌入层和位置编码层" class="headerlink" title="嵌入层和位置编码层"></a>嵌入层和位置编码层</h3><p>嵌入层和位置编码层将编码器输入和解码器输入序列变成向量表示.</p><p>####嵌入层<br>使用pytorch的Embedding()函数实现嵌入<br>####位置编码<br>位置编码是一个固定的矩阵大小是(序列长度seq_len,嵌入向量长度emb_size)<br>Transformer中使用的是不同频率的三角函数<br>$$ PE(pos,2i)=sin(\frac{pos}{10000^{2i/emb_size}})$$<br>$$ PE(pos,2i+1)=cos(\frac{pos}{10000^{2i/emb_size}})$$</p><p><strong>很多地方的代码在增加位置编码时给嵌入向量乘了sqrt(d_model)</strong></p><pre class="line-numbers language-none"><code class="language-none">class EmdAndPos(nn.Module):    &#39;&#39;&#39;    处理好的句子序列,并给他加上position编码    参数(emb_size&#x3D;d_model, seq_len, dict_number, padding_idx)    输入:(batch_size, seq_len)    输出:(batch_size, seq_len, emb_size)    test:    input1 &#x3D; torch.LongTensor([[1,2,4,5],[4,3,2,9]])    net &#x3D; nn.Sequential(EmdAndPos(16,4,10))    print(net(input1))    &#39;&#39;&#39;    def __init__(self,emb_size,seq_len,dict_number):        super(EmdAndPos, self).__init__()        self.emb &#x3D; nn.Embedding(num_embeddings&#x3D;dict_number, embedding_dim&#x3D;emb_size, padding_idx&#x3D;0)        self.pos &#x3D; self._position(emb_size,seq_len)    def _position(self,emb_size,seq_len):        &#39;&#39;&#39;        pos只与位置有关,没有学习过程,句子中的每一个单词产生一个描述位置的与词嵌入等长的向量,整个句子产生一个(seq_len,emb_size)的矩阵        输入:处理好的句子序列        输出:输出(seq_len , emb_size)的矩阵        &#39;&#39;&#39;        PE &#x3D; np.zeros((seq_len,emb_size))        def func(pos,i,emb_size):            if i%2 &#x3D;&#x3D;0:                return math.sin(pos &#x2F; (10000**(1.0*i&#x2F;emb_size)))            else:                return math.cos(pos &#x2F; (10000**(1.0*(i-1)&#x2F;emb_size)))        for xy,val in np.ndenumerate(PE):            PE[xy]&#x3D; func(xy[0], xy[1], emb_size)        pos_matrix &#x3D; torch.from_numpy(PE)        return pos_matrix    def forward(self, inputs):        X &#x3D; self.emb(inputs)        # print(self.pos)        # print(&quot;*&quot;*80)        # print(X)        return X+self.pos<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="基于点乘的多头注意力机制"><a href="#基于点乘的多头注意力机制" class="headerlink" title="基于点乘的多头注意力机制"></a>基于点乘的多头注意力机制</h3><p>多头注意力机制就是在原来点乘注意力机制的基础上,把原来d_model长度的向量切分成heads份,运算后在连接起来.它的好处是允许模型在不同的表示子空间里学习.在很多实验里发现,不同的表示空间的头捕获的信息是不同的.<br>对于上一层输入的X(batch_size, seq_len, d_model),先使用线性变换(没有激活函数)分别映射成QKV(batch_size, seq_len, d_model).需要注意在解码器中QKV的来源不用,Q来源于源语言,KV是目标语言的仿射变换.<br>之后对Q,K,V进行切分,切分的参数矩阵维度分别是(heads, d_model, d_k ),(heads, d_model, d_k )(heads, d_model, d_v );(d_k=d_v=d_model/heads),</p><pre class="line-numbers language-none"><code class="language-none">self.W_Q &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_k&#x2F;&#x2F;heads),requires_grad&#x3D;True)self.W_K &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_k&#x2F;&#x2F;heads),requires_grad&#x3D;True)self.W_V &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_v&#x2F;&#x2F;heads),requires_grad&#x3D;True)self.register_parameter(&#39;multihead_proj_weight&#39;, None)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这样切分后的qkv向量进行运算后在连接起来可以获得一个(1,heads<em>d_k)的向量并且(heads</em>d_k=d_model),对输出向量右乘一个输出矩阵W(d_model,d_model)获得最终多头注意力机制的score.<br><strong>但这样写的权重好像不会参与训练,所以我按照<a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html">d2l</a>里的多头网络写法改写了多头注意力的分片方法</strong></p><pre class="line-numbers language-none"><code class="language-none">class MultiHeadAttention(nn.Module):    &#39;&#39;&#39;     基于点乘的多头注意力层;    Q的维度(L,d_k),V的维度(L,d_k),V的维度(L,d_v);d_k,d_v分别表示key和value的大小,通常设置d_k&#x3D;d_v&#x3D;d_model    输入:(batch_size, seq_len, d_model)    输出:(batch_size, seq_len, d_model)    问题:dropout不知道在哪加    &#39;&#39;&#39;    def __init__(self, seq_len,heads, d_model, d_k&#x3D;None, d_v&#x3D;None, dropout&#x3D;0.1, decode&#x3D;False):        super(MultiHeadAttention, self).__init__()        self.d_k &#x3D; d_model if not d_k else d_k        self.d_v &#x3D; d_model if not d_v else d_v        self.heads &#x3D; heads        self.head_dim &#x3D; d_model &#x2F;&#x2F; heads        self.seq_len &#x3D; seq_len        assert self.head_dim * heads &#x3D;&#x3D; d_model ,&quot;heads必须能整除d_model&quot;        self.Q &#x3D; nn.Linear(d_model,d_k, bias&#x3D;False)        self.K &#x3D; nn.Linear(d_model,d_k, bias&#x3D;False)        self.V &#x3D; nn.Linear(d_model,d_v, bias&#x3D;False)        # 这样写的权重好像不参与训练,改成mxnet里的实现了        # self.W_Q &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_k&#x2F;&#x2F;heads),requires_grad&#x3D;True)        # self.W_K &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_k&#x2F;&#x2F;heads),requires_grad&#x3D;True)        # self.W_V &#x3D; nn.Parameter(data&#x3D;torch.tensor(heads, d_model, d_v&#x2F;&#x2F;heads),requires_grad&#x3D;True)        # self.register_parameter(&#39;multihead_proj_weight&#39;, None)        self.dropout &#x3D; nn.Dropout(dropout)        self.outputlinear &#x3D; nn.Linear(d_k,d_model)        self.decode &#x3D; decode        #解码器需要future-mask        if not decode:            self.mask &#x3D; None        else:            self.mask &#x3D; self._make_mask(seq_len).to(device)        self.softmax &#x3D; nn.Softmax(dim&#x3D;-1)                def _make_mask(self, dim):        matirx &#x3D; np.ones((dim, dim))        mask &#x3D; torch.Tensor(np.tril(matirx))        return mask&#x3D;&#x3D;0    def _dotmulAtt(self, q, k, v, mask):        &#39;&#39;&#39;        q,k,v向量点乘注意力        q,k,v输入维度(batch_size * heads,seq_len,head_dim)        返回维度:(batch_size * heads ,seq_len, head_dim)        &#39;&#39;&#39;        d &#x3D; q.shape[-1]        scores &#x3D; torch.bmm(q, k.transpose(1, 2)) &#x2F; math.sqrt(d)        # print(&quot;dot_att_scores&quot;)        # print(scores[0])        # print(&quot;*&quot;*80)        self.attention_weights &#x3D; self.softmax(scores.masked_fill(mask.unsqueeze(1).expand((-1, self.heads, -1, -1)).reshape(-1, self.seq_len, self.seq_len), value&#x3D;float(&quot;-inf&quot;)))                # print(&quot;dot_att_weights&quot;)        # #print(scores.masked_fill(mask.unsqueeze(1).expand((-1, self.heads, -1, -1)).reshape(-1, self.seq_len, self.seq_len), value&#x3D;float(&quot;-inf&quot;))[0])        # print(self.attention_weights[0])        # print(&quot;*&quot;*80)        return torch.bmm(self.dropout(self.attention_weights), v)    def _transpose_qkv(self, X, num_heads):         &#39;&#39;&#39;qkv变换分片,以引用多头注意力机制&#39;&#39;&#39;        X &#x3D; X.reshape(X.shape[0], X.shape[1], num_heads, -1)              X &#x3D; X.permute(0, 2, 1, 3)        return X.reshape(-1, X.shape[2], X.shape[3])    def _transpose_output(self, X, num_heads):        &#39;&#39;&#39;逆变换,使output和输入shape相同&#39;&#39;&#39;        X &#x3D; X.reshape(-1, num_heads, X.shape[1], X.shape[2])        X &#x3D; X.permute(0, 2, 1, 3)        return X.reshape(X.shape[0], X.shape[1], -1)        def get_attention_weights(self):        return self.attention_weights        def forward(self, q, k, v, mask):        # print(&quot;att_in&quot;)        # print(q)        # print(&quot;*&quot;*80)        # 将XY仿射变换成QKV        # t&#x3D;self.Q(q.float())        # print(t.dtype,t.shape)        # print(len(t))        # print(t[0].type, t[1].type)        # print(&quot;*&quot;*80)        Q &#x3D; self._transpose_qkv(self.Q(q), self.heads)        K &#x3D; self._transpose_qkv(self.K(k), self.heads)        V &#x3D; self._transpose_qkv(self.V(v), self.heads)        # print(&quot;att_in&quot;)        # print(Q)        # print(&quot;*&quot;*80)        #点积注意力,mask好像有bug        if self.decode:            #解码器,有future_mask和padding_mask            output &#x3D; self._dotmulAtt(Q, K, V, mask|self.mask)        else:            #编码器,只有padding_mask            output &#x3D; self._dotmulAtt(Q, K, V, mask)        #concat        output_concat &#x3D; self._transpose_output(output, self.heads)        # print(&quot;att_out&quot;)        # print(output_concat[0])        # print(&quot;*&quot;*80)        return self.outputlinear(output_concat)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>新的方法实现方法是:将变换后的QKV使用一个变换函数分片(而不是用不同的权重去成),然后直接应用点积注意力机制计算,最后再逆变换回原来的shape.通过一个线性层输出<br>最终输出的矩阵O大小应为(batch_size, seq_len, d_model),并且为了方便显示注意力机制也可以输出一个注意力权重矩阵.</p><h3 id="Mask操作"><a href="#Mask操作" class="headerlink" title="Mask操作"></a>Mask操作</h3><p>mask分为两部分</p><ul><li><p>句长掩码padding mask</p><p>在批量处理多个样本时(训练或解码) ,由于要对源语言和目标语言的输入进行批次化处理,而每个批次内序列的长度不一样,为了方便对批次内序列进行矩阵表示,需要进行对齐操作,即在较短的序列后面填充 0 来占位(padding 操作) 。而这些填充 0 的位置没有实际意义,不参与注意力机制的计算,因此,需要进行掩码操作,屏蔽其影响.</p></li><li><p>未来信息掩码 future mask,</p><p>对于解码器来说,由于在预测的时候是自左向右进行的, 即第 t 时刻解码器的输出只能依赖于 t 时刻之前的输出。 且为了保证训练解码一致,避免在训练过程中观测到目标语言端每个位置未来的信息,因此需要对未来信息进行屏蔽。具体的做法是:构造一个上三角值全为­inf 的 Mask矩阵, 也就是说, 在解码器计算中, 在当前位置, 通过未来信息掩码把序列之后的信息屏蔽掉了,避免了 t 时刻之后的位置对当前的计算产生影响.</p></li></ul><p>其中,future mask 只对解码器起作用,padding mask在embedding层前生成一个句子表mask,然后作为参数给多头注意力机制</p><pre class="line-numbers language-none"><code class="language-none"># paddingdef _make_padding_mask(self, seq, seq_len, pad&#x3D;0):        &#39;&#39;&#39;把idx的做成padding_mask&#39;&#39;&#39;        mask &#x3D; (seq&#x3D;&#x3D;pad)        mask.bool()        mask &#x3D; mask.unsqueeze(1).expand((-1, seq_len, -1))        return mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h3 id="Encode块"><a href="#Encode块" class="headerlink" title="Encode块"></a>Encode块</h3><p>结合前面写的部分,直接像拼积木一样拼起来就行了,论文里使用了6个encode块和6个decoder块,直接一个接一个连起来就行了</p><h3 id="Docoder"><a href="#Docoder" class="headerlink" title="Docoder"></a>Docoder</h3><p>解码器也类似Encoder堆积木,但是Encoder-decoder在写的时候有一些问题:</p><ul><li>这一模块的qkv不同,q是解码器每个位置的表示,kv变成了编码器每个位置的表示</li></ul><h2 id="3-模型的训练"><a href="#3-模型的训练" class="headerlink" title="3.模型的训练"></a>3.模型的训练</h2><p>因为没有显卡资源,只能把模型放到Colab上运行,只进行了参数调整</p><pre class="line-numbers language-none"><code class="language-none">testloss &#x3D; 0batch_size &#x3D; 1024lr &#x3D; 0.0001dict_number &#x3D; 10148epochs &#x3D; 20n_layers &#x3D; 6seq_len &#x3D; 32heads &#x3D;4d_model &#x3D; 512hidden_dim &#x3D; 1024norm_shape &#x3D; [seq_len, d_model]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="wramup训练"><a href="#wramup训练" class="headerlink" title="wramup训练"></a>wramup训练</h3><p>动态变化学习率,在刚开始训练时大幅降低学习率,随epoch减弱减低幅度.<br>可以帮助模型更快的收敛而不是不断震荡</p><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>虽然句子长度选的短就32截断了很多句子,但是感觉ppl = math.exp(total_loss / count_loss)的计算有点问题(后来检查了,确实有问题,输出结果特别拉很反常)</p><pre class="line-numbers language-none"><code class="language-none">Validating at epoch 0002: loss: 9.050756, ppl: 8524.98step&#x3D; 15Validating at epoch 0003: loss: 7.819238, ppl: 2488.01step&#x3D; 15Validating at epoch 0004: loss: 6.188854, ppl: 487.287,step&#x3D; 15Validating at epoch 0005: loss: 5.540467, ppl: 254.797,step&#x3D; 15Validating at epoch 0006: loss: 4.841723, ppl: 126.687,step&#x3D; 15Validating at epoch 0007: loss: 4.001035, ppl: 54.6547,step&#x3D; 15Validating at epoch 0008: loss: 3.073742, ppl: 21.6227,step&#x3D; 15Validating at epoch 0009: loss: 2.307232, ppl: 10.0466,step&#x3D; 15Validating at epoch 0010: loss: 1.733115, ppl: 5.65825,step&#x3D; 15Validating at epoch 0011: loss: 1.251509, ppl: 3.49561,step&#x3D; 15Validating at epoch 0012: loss: 0.794980, ppl: 2.2144,step&#x3D; 15Validating at epoch 0013: loss: 0.495874, ppl: 1.64193,step&#x3D; 15Validating at epoch 0014: loss: 0.315098, ppl: 1.37039,step&#x3D; 15Validating at epoch 0015: loss: 0.225537, ppl: 1.253,step&#x3D; 15<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>感觉seq_len选太小了,有很大问题</p><pre class="line-numbers language-none"><code class="language-none">输出语句1i in that 50 spring income dna meaning blue spring street is &amp;#93; policy &lt;eos&gt; , &lt;eos&gt; , and by &lt;eos&gt; , , give to a the of . &amp;apos;s war and目标语句1and now we would like to introdu@@ ce you to jo@@ ey .输出语句2, gets dna source — fully security — changing gas office — star — known capacity complicated bor@@ &lt;eos&gt; glo@@ order street oil fish &amp;apos;s we the new the . given and目标语句2so today , i &amp;apos;m going to tell you about some people who didn &amp;apos;t move out of their neighbor@@ ho@@ ods .<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="完成项目时遇到的困难和感想"><a href="#完成项目时遇到的困难和感想" class="headerlink" title="完成项目时遇到的困难和感想"></a>完成项目时遇到的困难和感想</h2><p>这是本人的第一个NLP的深度学习项目,写一个Transformer模型对我来说是一个相当有挑战型的问题,不出意外的是做这个项目的时候出现了各种各样的问题,</p><ul><li>bpevocab表中的字符不全,有些在验证集中的字符缺失</li><li>在写多头神经网络时,发现matmul,bmm和@对不同tensor.shape的效果是不同的,</li><li>对多头注意力机制的理解错误,一开始把注意力权重当成类似全连接层的权重了,把qkv分片在使用不同的权重求注意力得分,后来看了网上别人的论文解读和代码才发现这个错误,</li><li>padding mask,这个mask因为理解错误,我把mask做成了对称矩阵,于是在softmax中依然会有一行是[nan,…nan],导致全部tensor变成nan</li><li>我在写生成位置向量函数时使用了np和tensor.form_numpy(),而np生成的是64位的数,tensor.form_numpy()的dtype是float64(很无语),于是出现了数据类型不一致的问题而反复报错,最后使用tensor.form_numpy().float()让数据一致</li><li>AddNorm,这个层我后来才发现其实可以不用写一个类,当初也想了半天,不知道怎么把前馈网络和多头注意力层放到AddNorm中间残差,这明显是经验不足的问题,又浪费时间又写了没用的代码.</li><li>bpe分词我现在还不会,测试集不知道怎么分成bpe,模型跑出来的句子还是分词后的,不知道怎么变成完整的句子</li><li>没有显卡,gpu调试很不方便,而且只能白嫖colab训练,出了问题得先在本地找问题改代码,然后push到github,然后再colab上Git clone,太折磨了</li><li>模型里新生成的tensor要放入对应的device里,不然也会有报错,比如在GPU上跑是数据在GPU上,此时新生成的位置编码默认在cpu上,会发生错误</li><li>以前没写过NLP,模型train和eval直接抄别人的了,测试集上的ppl因为分词问题搞不出来</li><li>paperswithcode上MT的指标好像是BLEU,不过我不会</li><li>可能是初始化的问题第一轮ppl达到了8000多,原因未知</li></ul><p>##参考文献</p><p>[1]Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.<br>[2]肖桐 朱靖波，机器翻译：基础与模型，电子工业出版社, 2021.<br>[3]<a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html">https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test:article title</title>
      <link href="/2021/11/04/test-article-title/"/>
      <url>/2021/11/04/test-article-title/</url>
      
        <content type="html"><![CDATA[<p>#知识图谱在医疗图像领域的使用</p><p>目前,知识图谱被广泛用于搜索引擎,推荐算法,在智能医疗方向也有所发展. 我今后研究的方向可能是图网络和医疗图像处理,本篇文章主要分析总结了知识图谱在图像分割等传统视觉领域的应用和医疗图像领域的应用.</p><h2 id="知识图谱在传统视觉领域的应用"><a href="#知识图谱在传统视觉领域的应用" class="headerlink" title="知识图谱在传统视觉领域的应用"></a>知识图谱在传统视觉领域的应用</h2><p>通过结合知识图谱,可以将将视觉任务从纯感知层转向认知层，也就是具有一定视觉推理能力。目前这些应用包括看图说话、视觉常识生成、视觉问答、场景图生成、手势识别、多模态检索和视觉语言导航任务。</p><p>我认为在上述任务中可以应用在医疗图像中的有看图说话和视觉常识生成,下面介绍这两个任务:</p><h3 id="看图说话"><a href="#看图说话" class="headerlink" title="看图说话"></a>看图说话</h3><p>[1]中研究了实体感知的图像描述（看图说话），旨在通过利用相关文章中的背景知识来描述与图像相关的命名实体和事件。由于命名实体的长尾分布，很难建立命名实体和视觉信息之间的关联。此外，从背景文章中提取细粒度的实体和关系以生成有关图像的描述非常具有挑战性。为了应对这些挑战，这项研究构建了一个多模态知识图谱，将视觉对象与命名实体相关联，并从网络收集的外部知识中同时建立实体之间的关系，然后对多模态知识图谱通过图注意力机制集成到字幕生成模型中。</p><h3 id="视觉常识生成"><a href="#视觉常识生成" class="headerlink" title="视觉常识生成"></a>视觉常识生成</h3><p>[2]中研究的是2020年新提出的视觉常识生成任务，比看图说话要更难，因为需要常识推理来预测给定图像之前或之后的事件。论文提出了一个知识增强的多模态 BART (KM-BART)模型，这是一种基于 Transformer 的seq2seq模型，能够从图像和文本的多模态输入中推理常识知识。具体的，这项研究将生成式 BART（注意是BART而不是BERT） 架构调整为具有视觉和文本输入的多模态模型，并进一步开发了新颖的预训练任务，以提高视觉常识生成 (VCG) 任务的模型性能。特别的，基于知识的常识生成 (KCG) 的预训练任务通过利用在外部常识知识图谱上预训练的大型语言模型中的常识知识，提高了 VCG 任务的模型性能. </p><h2 id="知识图谱在智能医疗的应用"><a href="#知识图谱在智能医疗的应用" class="headerlink" title="知识图谱在智能医疗的应用"></a>知识图谱在智能医疗的应用</h2><p>随着智能时代的到来，把临床数据、临床指南、组学数据通过大数据和知识图谱结合，核心医学概念的全面覆盖、医疗生态圈内全方位知识数据的聚合，构建综合智能医疗系统，给临床医生、患者和科研工作者等提供帮助，成为未来医疗的发展方向。目前,医学是知识图谱应用最广的垂直领域之一[3],在如疾病风险评估、智能辅助诊疗、医疗质量控制及医疗知识问答等智慧医疗领域都有着很好的发展前景. 已经有很多公司构建了自己的知识图谱，如IBM 的 Watson Health、阿里健康的“医知鹿”医学智库、搜狗的 AI 医学知识图谱 APGC 等医学知识图谱的应用近年来也开始进入人们视线.</p><p>显然,这类应用与医疗图像明显任务不同,下面我将介绍知识图谱在医疗图像上的应用</p><h2 id="知识图谱在医疗图像领域的应用"><a href="#知识图谱在医疗图像领域的应用" class="headerlink" title="知识图谱在医疗图像领域的应用"></a>知识图谱在医疗图像领域的应用</h2><p>人工智能处理医学影像，是指将人工智能技术具体应用在医学影像的诊断上。智能医学影像被业内人士认为是最有可能率先实现商业化的人工智能医疗领域，一方面是因为医疗领域这些年里积累了十分可观的数据量，有助于人工智能算法的训练；另一方面医疗影像在医疗诊断中的占比很大，根据数据显示，目前医疗数据中有超过90％来自医疗影像，医疗影像数据已经成为医生诊断必不可少的依据之一。人工智能在医学影像领域的应用，解决了不少亟需解决的痛点，其中最突出的有以下几点：<strong>痛点一、医学影像领域专业医生缺口大</strong> <strong>痛点二、医学影像诊断速度有限</strong> <strong>痛点三、医学影像误诊漏诊率高</strong>[4]</p><p>从医疗影像领域拓展到整个影像处理领域，人工智能在处理影像方面的瓶颈具体表现在两点，第一，图像的视觉表达和语义之间很难建立合理关联，描述实体间产生巨大的语义鸿沟(Semantic Gap)；第二，语义本身具有表达的多义性和不确定性。目前，已经有越来越多的研究在关注上述瓶颈，并致力于有效模型和方法以实现理解印象中的语义表达。<br>第一个问题专注于对图像的认知,类似前文提及的看图说话和视觉常识生成.<br>对于第二个问题，图像的语义分析是建立在图像的信息抽取技术上的，图像信息抽取研究经历了三个阶段：利用文本来描述图像特征——图像底层视觉特征——图像的语义内在特征。<br>早期的图像信息抽取是利用底层图像特征，如方向梯度直方图HOG和尺度不变特征转换SIFT。基于机器学习的方法从最简单的像素级别阈值法、基于像素聚类的分割方法到基于图论划分的分割方法。基于机器学习的方法，先将输入图像分为一些独立的区域块，并提取每个区域块的特征，然后根据一定的规则建立图像特征与语义类别之间的概率模型，建立起能量函数，并通过手工标注的特征库，迭代计算对能量函数进行优化，得到最优的参数，最终得到图像信息模型。这种机器学习的方法过于依赖手工标注的特征库，难以广泛表示图像特征，在实际应用中有很大的局限性。<br>面对海量的图像信息，人们期望以更加智能的方式组织图像资源。为了满足这种需求，知识图谱应运而生，它们力求通过将知识进行更加有序、有机的组织，对用户提供更加智能的访问接口，使用户可以更加快速、准确地访问自己需要的知识信息，并进行一定的知识挖掘和智能决策。<br>知识图谱即为用图对知识和知识间关系进行建模。知识图谱的功能主要体现在知识组织、展示与搜索方面：第一，在一定程度上克服自然语言的歧义性; 第二，把经过梳理、总结的知识提供给用户; 第三，提供更深入更广阔的知识，知识图谱尝试通过对其他用户相关的搜索记录进行推理，激发用户对知识的搜索兴趣，从而进行一次全新的查询操作。知识图谱技术的出现使得信息可以在语义层面上进行整合，这种语义层次的关联技术能够为图像的语义分析研判提供强有力的支撑，从而更准确地提取图像中所包含的信息。</p><h3 id="知识图谱处理影像过程"><a href="#知识图谱处理影像过程" class="headerlink" title="知识图谱处理影像过程"></a>知识图谱处理影像过程</h3><p>构建图像处理领域的知识图谱以完成对影像的语义分析，其过程主要包括以下六个步骤——数据获取、信息抽取、知识融合加工、知识存储、知识应用以及可视化结果。<br>图像数据获取是采集原始的图像数据，即可以包含结构化数据，如标注文本等信息，也可以包含半结构化和非结构化的图像特征信息等；图像的信息抽取是将图像数据进行抽象归纳为更具有语义特点的单元，例如实体抽取、实体之间的关系抽取以及实体的属性抽取等，为后续的知识提取做铺垫；图像的知识融合和加工是在图像信息抽取的基础上将信息升级为知识，例如具有相同表达但不同信息的实体消岐、具有相同意思但不同表达的知识合并、对知识进行概念归纳的本体构建以及丰富语义内涵的知识推理；图像的知识存储是利用知识图谱的三元组表达方式，结合前面两步获取的知识，将其存储在数据库中并进行知识更新；可视化结果则是利用知识图谱的可视化工具将图像的知识图像语义分析记过以网状可视化方式直观的展现出图像知识组成。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/11/04/hello-world/"/>
      <url>/2021/11/04/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
